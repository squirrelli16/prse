{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/squirrelli16/prse/blob/main/rec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0aUfrF-wIC8w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aUfrF-wIC8w",
        "outputId": "69fb4cff-d127-4fa2-9271-606c836bdf2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask_cors in /usr/local/lib/python3.10/dist-packages (5.0.0)\n",
            "Requirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.10/dist-packages (from flask_cors) (3.0.3)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask_cors) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask_cors) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask_cors) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask_cors) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask_cors) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->Flask>=0.9->flask_cors) (3.0.2)\n",
            "Requirement already satisfied: underthesea in /usr/local/lib/python3.10/dist-packages (6.8.4)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.7)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.10/dist-packages (from underthesea) (0.9.11)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.32.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.5.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0.2)\n",
            "Requirement already satisfied: underthesea-core==1.0.4 in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.0.4)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2024.9.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2024.8.30)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.5.0)\n",
            "Requirement already satisfied: waitress in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.11.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Collecting mysql-connector-python\n",
            "  Downloading mysql_connector_python-9.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
            "Downloading mysql_connector_python-9.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mysql-connector-python\n",
            "Successfully installed mysql-connector-python-9.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install flask_cors\n",
        "!pip install underthesea\n",
        "!pip install waitress\n",
        "!pip install pyngrok\n",
        "!pip install openai==0.28\n",
        "!pip install sentence-transformers\n",
        "!pip install mysql-connector-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YKJ4oG-WHkeK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKJ4oG-WHkeK",
        "outputId": "89dcf835-aa0e-4df0-ae19-4d753591ec1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "13b03f9e38ff0e2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13b03f9e38ff0e2f",
        "outputId": "b88734fb-46c1-4541-da49-82e8457262d6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Flask app is available at: https://ad62-34-19-104-54.ngrok-free.app\n",
            "API_site: https://ad62-34-19-104-54.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Dec/2024 13:33:33] \"POST /rec_chatbot HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Dec/2024 13:33:45] \"\u001b[33mGET /robots.txt HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Dec/2024 13:33:45] \"\u001b[31m\u001b[1mGET /rec_chatbot HTTP/1.1\u001b[0m\" 405 -\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Dec/2024 13:34:14] \"OPTIONS /rec_chatbot HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Dec/2024 13:34:37] \"POST /rec_chatbot HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Dec/2024 13:34:51] \"OPTIONS /rec_chatbot HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Dec/2024 13:35:21] \"POST /rec_chatbot HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Dec/2024 13:38:28] \"OPTIONS /rec_chatbot HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Dec/2024 13:38:54] \"POST /rec_chatbot HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from underthesea import word_tokenize\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from waitress import serve\n",
        "from pyngrok import ngrok, conf\n",
        "from google.colab import userdata\n",
        "import pandas as pd\n",
        "import openai\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import mysql.connector\n",
        "import numpy as np\n",
        "import random\n",
        "app = Flask(__name__)\n",
        "conf.get_default().auth_token = userdata.get('Ngrok')\n",
        "ngrok.set_auth_token('Ngrok')  # Ensure you have the correct token\n",
        "ngrok.kill()\n",
        "public_url = ngrok.connect(5000).public_url\n",
        "print(f'Flask app is available at: {public_url}')\n",
        "CORS(app)\n",
        "def preprocess_vietnamese(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return ' '.join(tokens)\n",
        "def format_price(price):\n",
        "    return \"{:,.0f}\".format(price).replace(',', '.')\n",
        "\n",
        "def get_effective_price(row, courses_discount):\n",
        "    \"\"\"\n",
        "    Get the effective price considering discounts\n",
        "    \"\"\"\n",
        "    course_id = row['id']\n",
        "    original_price = row['original_price']\n",
        "\n",
        "    # Check if course has an active discount\n",
        "    discount_info = courses_discount[\n",
        "        (courses_discount['course_id'] == course_id) &\n",
        "        (courses_discount['is_active'] == True)\n",
        "    ]\n",
        "\n",
        "    if not discount_info.empty:\n",
        "        return discount_info.iloc[0]['discount_price']\n",
        "    return original_price\n",
        "\n",
        "def calculate_combined_similarity(courses, courses_discount, text_weight=0.6, rating_weight=0.2, price_weight=0.2, category_weight=0.9):\n",
        "    \"\"\"\n",
        "    Calculate similarity between courses based on description, rating, and effective price\n",
        "\n",
        "    Parameters:\n",
        "    - courses: DataFrame containing 'id', 'description', 'average_rating', and 'original_price'\n",
        "    - courses_discount: DataFrame containing discount information\n",
        "    - text_weight: weight for description similarity (default: 0.6)\n",
        "    - rating_weight: weight for rating similarity (default: 0.2)\n",
        "    - price_weight: weight for price similarity (default: 0.2)\n",
        "\n",
        "    Returns:\n",
        "    - combined similarity matrix\n",
        "    \"\"\"\n",
        "    # Preprocess description\n",
        "    courses['description'] = courses['description'].apply(preprocess_vietnamese)\n",
        "\n",
        "    # Calculate text similarity\n",
        "    tfidf = TfidfVectorizer(\n",
        "        max_features=3522,\n",
        "        ngram_range=(1, 2)\n",
        "    )\n",
        "    text_vectors = tfidf.fit_transform(courses['description'])\n",
        "    text_similarity = cosine_similarity(text_vectors)\n",
        "\n",
        "    # Calculate effective prices considering discounts\n",
        "    courses['effective_price'] = courses.apply(\n",
        "        lambda row: get_effective_price(row, courses_discount),\n",
        "        axis=1\n",
        "    )\n",
        "    # Normalize numerical features\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Handle rating similarity\n",
        "    ratings = courses[['average_rating']].values\n",
        "    normalized_ratings = scaler.fit_transform(ratings)\n",
        "    rating_similarity = 1 - np.abs(normalized_ratings - normalized_ratings.T)\n",
        "\n",
        "    # Handle price similarity using effective prices\n",
        "    prices = courses[['effective_price']].values\n",
        "    normalized_prices = scaler.fit_transform(prices)\n",
        "    price_similarity = 1 - np.abs(normalized_prices - normalized_prices.T)\n",
        "\n",
        "    # Handle category similarity: 1 if categories match, 0 if they don't\n",
        "    categories = courses[['sub_category_id']].values\n",
        "    category_similarity = np.array([[1 if cat1 == cat2 else 0 for cat2 in categories] for cat1 in categories])\n",
        "\n",
        "    # Combine all similarities with their respective weights\n",
        "    combined_similarity = (\n",
        "        text_weight * text_similarity +\n",
        "        rating_weight * rating_similarity +\n",
        "        price_weight * price_similarity +\n",
        "        category_weight * category_similarity\n",
        "    )\n",
        "\n",
        "    return combined_similarity\n",
        "\n",
        "# Load data\n",
        "courses = pd.read_csv('drive/MyDrive/Data_PRSE/course.csv')\n",
        "courses_discount = pd.read_csv('drive/MyDrive/Data_PRSE/course_discount.csv')\n",
        "courses_category = pd.read_csv('drive/MyDrive/Data_PRSE/course_category.csv')\n",
        "\n",
        "\n",
        "# Load PhoBERT model and tokenizer\n",
        "model_name = 'vinai/phobert-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# S·ª≠ d·ª•ng SQLAlchemy ƒë·ªÉ d·ªÖ d√†ng chuy·ªÉn ƒë·ªïi sang DataFrame\n",
        "connection = mysql.connector.connect(\n",
        "    host='14.225.253.200',       # IP address of your DB instance\n",
        "    user='root',\n",
        "    password='Hoanganh123!@#',\n",
        "    database='prse'\n",
        ")\n",
        "cursor = connection.cursor()\n",
        "\n",
        "# Thay ƒë·ªïi c√¢u query theo c·∫•u tr√∫c database c·ªßa b·∫°n\n",
        "query = \"\"\"\n",
        "    SELECT id, title, description, original_price\n",
        "    FROM course\n",
        "    WHERE is_publish = 1\n",
        "\"\"\"\n",
        "\n",
        "cursor.execute(query)\n",
        "\n",
        "# Fetch all results\n",
        "data = cursor.fetchall()\n",
        "\n",
        "# Get column names\n",
        "columns = [desc[0] for desc in cursor.description]\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "# Print the DataFrame\n",
        "df['name']= df['title']\n",
        "df['title'] = df['title'].str.lower() + \". \" + df['description'].str.lower()\n",
        "# Function to generate embeddings from PhoBERT\n",
        "def generate_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
        "    return embeddings.squeeze().numpy()\n",
        "\n",
        "# Function to find courses based on user input\n",
        "def find_course(user_input, max_results=3):\n",
        "    user_input= user_input.lower()\n",
        "    user_input_embedding = generate_embedding(user_input)\n",
        "\n",
        "    course_titles = df['title'].tolist()\n",
        "    course_embeddings = np.array([generate_embedding(title) for title in course_titles])\n",
        "\n",
        "    cosine_scores = np.dot(course_embeddings, user_input_embedding) / (np.linalg.norm(course_embeddings, axis=1) * np.linalg.norm(user_input_embedding))\n",
        "\n",
        "    top_results = np.argsort(cosine_scores)[-max_results:][::-1]\n",
        "\n",
        "    valid_results = [i for i in top_results if cosine_scores[i] >= 0.55]\n",
        "\n",
        "    if not valid_results:\n",
        "        return \"Kh√¥ng t√¨m th·∫•y kh√≥a h·ªçc ph√π h·ª£p.\"\n",
        "    else:\n",
        "        # Retrieve courses that pass the cosine score threshold\n",
        "        top_courses = df.iloc[valid_results]\n",
        "        return top_courses\n",
        "# Select required columns\n",
        "courses = courses[['id', 'average_rating', 'description', 'original_price']]\n",
        "courses = courses.merge(courses_category[['course_id', 'sub_category_id']], left_on='id', right_on='course_id', how='left')\n",
        "\n",
        "# Calculate similarity matrix with effective prices\n",
        "similarity_matrix = calculate_combined_similarity(\n",
        "    courses,\n",
        "    courses_discount,\n",
        "    text_weight=0.75,\n",
        "    rating_weight=0.5,\n",
        "    price_weight=0.4\n",
        ")\n",
        "# Assuming you have these loaded from somewhere\n",
        "# You might want to load these when the application starts\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    Load the necessary data for recommendations\n",
        "    You'll need to implement this based on how you store your data\n",
        "    \"\"\"\n",
        "    global courses, similarity_matrix\n",
        "    # Load your courses DataFrame\n",
        "    # Load your similarity matrix\n",
        "    pass\n",
        "def get_top_rated_courses(n=10):\n",
        "    \"\"\"\n",
        "    Get top N courses with highest ratings\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Sort by rating and get top N courses\n",
        "        top_courses = courses.nlargest(n, 'average_rating')[['id', 'average_rating']]\n",
        "        top_courses['based_on_courses'] = 'top_rated'  # Indicate these are top rated courses\n",
        "\n",
        "        return top_courses\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting top rated courses: {e}\")\n",
        "        return None\n",
        "\n",
        "@app.route('/api/recommend', methods=['POST'])\n",
        "def get_recommendations():\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        course_ids = data.get('course_ids', [])\n",
        "\n",
        "        if not course_ids:\n",
        "            top_rated = get_top_rated_courses(10)\n",
        "            if top_rated is None:\n",
        "                return jsonify({\n",
        "                    'error': 'Could not fetch recommendations or top rated courses'\n",
        "                }), 500\n",
        "\n",
        "            result = top_rated.to_dict(orient='records')\n",
        "            return jsonify({\n",
        "                'status': 'success',\n",
        "                'recommendations': result,\n",
        "                'recommendation_type': 'top_rated'\n",
        "            })\n",
        "\n",
        "        # Get recommendations\n",
        "        recommendations = get_similar_courses_combined(course_ids)\n",
        "\n",
        "        if recommendations is None:\n",
        "            # Get top rated courses instead\n",
        "            top_rated = get_top_rated_courses(10)\n",
        "            if top_rated is None:\n",
        "                return jsonify({\n",
        "                    'error': 'Could not fetch recommendations or top rated courses'\n",
        "                }), 500\n",
        "\n",
        "            result = top_rated.to_dict(orient='records')\n",
        "            return jsonify({\n",
        "                'status': 'success',\n",
        "                'recommendations': result,\n",
        "                'recommendation_type': 'top_rated'\n",
        "            })\n",
        "\n",
        "        # Convert DataFrame to dictionary for JSON response\n",
        "        result = recommendations.to_dict(orient='records')\n",
        "\n",
        "        return jsonify({\n",
        "            'status': 'success',\n",
        "            'recommendations': result,\n",
        "            'recommendation_type': 'similar'\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\n",
        "            'error': str(e)\n",
        "        }), 500\n",
        "\n",
        "\n",
        "def get_similar_courses_combined(course_ids, n=10):\n",
        "    \"\"\"\n",
        "    Your existing recommendation function\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get indices for all input course IDs\n",
        "        indices = [courses.index[courses['id'] == cid][0] for cid in course_ids]\n",
        "\n",
        "        # Extract vectors for each course\n",
        "        course_vectors = similarity_matrix[indices]\n",
        "\n",
        "        # Combine vectors by taking the mean\n",
        "        combined_vector = np.mean(course_vectors, axis=0)\n",
        "\n",
        "        # Calculate similarity between combined vector and all courses\n",
        "        similarities = cosine_similarity([combined_vector], similarity_matrix)[0]\n",
        "\n",
        "        # Get indices of top N similar courses\n",
        "        # Exclude the input courses themselves\n",
        "        mask = np.ones(len(similarities), dtype=bool)\n",
        "        mask[indices] = False\n",
        "        filtered_similarities = similarities * mask\n",
        "\n",
        "        similar_indices = filtered_similarities.argsort()[::-1][:n]\n",
        "\n",
        "        # Create DataFrame with similar courses\n",
        "        similar_courses = courses.iloc[similar_indices][['id']]\n",
        "\n",
        "        # Add original course IDs for reference\n",
        "\n",
        "        return similar_courses\n",
        "\n",
        "    except IndexError as e:\n",
        "        print(f\"Error: One or more course IDs not found in dataset\")\n",
        "        return None\n",
        "\n",
        "# Add some error handlers\n",
        "@app.errorhandler(404)\n",
        "def not_found_error(error):\n",
        "    return jsonify({\n",
        "        'error': 'Resource not found'\n",
        "    }), 404\n",
        "\n",
        "@app.errorhandler(500)\n",
        "def internal_error(error):\n",
        "    return jsonify({\n",
        "        'error': 'Internal server error'\n",
        "    }), 500\n",
        "\n",
        "# Add a health check endpoint\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health_check():\n",
        "    return jsonify({\n",
        "        'status': 'healthy',\n",
        "        'message': 'Service is running'\n",
        "    })\n",
        "@app.route('/rec_chatbot', methods=['POST'])\n",
        "def find_courses():\n",
        "    data = request.get_json()\n",
        "    user_input = data.get('message', '')\n",
        "\n",
        "    # Validate input\n",
        "    if not user_input:\n",
        "        return jsonify({'error': 'No user input provided'}), 400\n",
        "\n",
        "    matching_courses = find_course(user_input)\n",
        "\n",
        "    if isinstance(matching_courses, pd.DataFrame) and not matching_courses.empty:\n",
        "        openai.api_key = userdata.get('GPT') # Replace with your OpenAI API Key\n",
        "        footer_responses = [\n",
        "            \"üåü H√£y c√πng kh√°m ph√° v√† b·∫Øt ƒë·∫ßu h√†nh tr√¨nh h·ªçc t·∫≠p th√∫ v·ªã v·ªõi EasyEdu nh√©!\",\n",
        "            \"üìö Ch√∫ng ta h√£y b∆∞·ªõc v√†o th·∫ø gi·ªõi ki·∫øn th·ª©c ƒë·∫ßy m√†u s·∫Øc c√πng EasyEdu n√†o!\",\n",
        "            \"üöÄ ƒê·ª´ng ng·∫ßn ng·∫°i, h√£y c√πng EasyEdu chinh ph·ª•c nh·ªØng ƒëi·ªÅu m·ªõi m·∫ª nh√©!\",\n",
        "            \"ü§ù H√£y ƒë·ªÉ EasyEdu ƒë·ªìng h√†nh c√πng b·∫°n trong cu·ªôc h√†nh tr√¨nh h·ªçc t·∫≠p tuy·ªát v·ªùi n√†y!\",\n",
        "            \"üåà Kh√°m ph√° nh·ªØng ƒëi·ªÅu tuy·ªát v·ªùi v√† b·∫Øt ƒë·∫ßu h·ªçc c√πng EasyEdu n√†o!\",\n",
        "            \"‚ú® Ch√∫ng m√¨nh h√£y c√πng nhau kh√°m ph√° h√†nh tr√¨nh h·ªçc t·∫≠p th√∫ v·ªã v·ªõi EasyEdu nh√©!\",\n",
        "            \"üìñ H√£y tham gia c√πng EasyEdu ƒë·ªÉ tr·∫£i nghi·ªám nh·ªØng b√†i h·ªçc b·ªï √≠ch n√†o!\",\n",
        "            \"üåü Kh·ªüi ƒë·∫ßu h√†nh tr√¨nh h·ªçc t·∫≠p c·ªßa b·∫°n v·ªõi EasyEdu ngay h√¥m nay!\",\n",
        "            \"üí° H√£y ƒë·ªÉ EasyEdu gi√∫p b·∫°n m·ªü r·ªông ki·∫øn th·ª©c v√† k·ªπ nƒÉng nh√©!\",\n",
        "            \"üéâ Ch√∫ng ta h√£y c√πng nhau h·ªçc h·ªèi v√† ph√°t tri·ªÉn v·ªõi EasyEdu n√†o!\",\n",
        "            \"üåç H√£y kh√°m ph√° th·∫ø gi·ªõi h·ªçc t·∫≠p r·ªông l·ªõn c√πng EasyEdu nh√©!\",\n",
        "            \"üåû Ch√∫ng ta h√£y c√πng EasyEdu x√¢y d·ª±ng t∆∞∆°ng lai t∆∞∆°i s√°ng h∆°n!\",\n",
        "            \"üìÖ H√£y b·∫Øt ƒë·∫ßu h√†nh tr√¨nh h·ªçc t·∫≠p c·ªßa b·∫°n v·ªõi nh·ªØng kh√≥a h·ªçc th√∫ v·ªã t·ª´ EasyEdu!\",\n",
        "            \"üéà Kh√°m ph√° v√† tr·∫£i nghi·ªám nh·ªØng ƒëi·ªÅu m·ªõi m·∫ª c√πng EasyEdu n√†o!\",\n",
        "            \"üó∫Ô∏è H√£y ƒë·ªÉ EasyEdu d·∫´n d·∫Øt b·∫°n tr√™n con ƒë∆∞·ªùng tri th·ª©c!\",\n",
        "            \"üéì C√πng EasyEdu t·∫°o n√™n nh·ªØng k·ª∑ ni·ªám h·ªçc t·∫≠p ƒë√°ng nh·ªõ nh√©!\",\n",
        "            \"üìù Kh√°m ph√° nh·ªØng kh√≥a h·ªçc h·∫•p d·∫´n v√† th√∫ v·ªã v·ªõi EasyEdu!\",\n",
        "            \"üîç H√£y tham gia c√πng EasyEdu ƒë·ªÉ kh√°m ph√° nh·ªØng ki·∫øn th·ª©c m·ªõi m·∫ª!\",\n",
        "            \"üèÜ Ch√∫ng ta h√£y c√πng nhau v∆∞∆°n t·ªõi nh·ªØng ƒë·ªânh cao tri th·ª©c c√πng EasyEdu!\",\n",
        "            \"üåº H√£y b·∫Øt tay v√†o h√†nh tr√¨nh h·ªçc t·∫≠p ƒë·∫ßy th√∫ v·ªã v·ªõi EasyEdu n√†o!\"\n",
        "        ]\n",
        "        header_responses = [\n",
        "            \"üåü Ch√∫ng t√¥i ƒë√£ t√¨m th·∫•y nh·ªØng kh√≥a h·ªçc th√∫ v·ªã ƒëang ch·ªù b·∫°n! H√£y xem ngay nh√©:\",\n",
        "            \"üéâ Tr·ª£ l√Ω EasyEdu ƒë√£ ph√°t hi·ªán ra c√°c kh√≥a h·ªçc ƒë·ªôc ƒë√°o d√†nh cho b·∫°n. H√£y c√πng kh√°m ph√° n√†o:\",\n",
        "            \"üìö Tr·ª£ l√Ω EasyEdu r·∫•t vui khi t√¨m th·∫•y c√°c kh√≥a h·ªçc ph√π h·ª£p v·ªõi b·∫°n! H√£y th·ª≠ ngay nh√©:\",\n",
        "            \"‚ú® Wow! Nh·ªØng kh√≥a h·ªçc tuy·ªát v·ªùi ƒë√£ s·∫µn s√†ng cho b·∫°n. H√£y xem ngay n√†o:\",\n",
        "            \"üöÄ Tr·ª£ l√Ω EasyEdu ƒë√£ t√¨m ra c√°c kh√≥a h·ªçc ph√π h·ª£p v·ªõi s·ªü th√≠ch c·ªßa b·∫°n. H√£y kh√°m ph√° nh√©:\",\n",
        "            \"üåº Tr·ª£ l√Ω EasyEdu r·∫•t vui th√¥ng b√°o r·∫±ng ƒë√£ c√≥ nh·ªØng kh√≥a h·ªçc tuy·ªát v·ªùi cho b·∫°n. H√£y xem ngay:\",\n",
        "            \"üéà Hooray! C√°c kh√≥a h·ªçc l√Ω t∆∞·ªüng ƒëang ch·ªù ƒë√≥n b·∫°n. H√£y t√¨m hi·ªÉu ngay nh√©:\",\n",
        "            \"üòç Tr·ª£ l√Ω EasyEdu ƒë√£ ph√°t hi·ªán ra nh·ªØng kh√≥a h·ªçc ƒë·∫∑c bi·ªát d√†nh cho b·∫°n. H√£y c√πng xem n√†o:\",\n",
        "            \"üíñ Nh·ªØng kh√≥a h·ªçc ph√π h·ª£p ƒë√£ ƒë∆∞·ª£c t√¨m th·∫•y! H√£y kh√°m ph√° c√πng tr·ª£ l√Ω EasyEdu nh√©:\",\n",
        "            \"üåà Tr·ª£ l√Ω EasyEdu r·∫•t vui khi th√¥ng b√°o r·∫±ng ƒë√£ c√≥ c√°c kh√≥a h·ªçc ph√π h·ª£p v·ªõi b·∫°n. H√£y xem ngay:\",\n",
        "            \"ü•≥ Tr·ª£ l√Ω EasyEdu ƒë√£ t√¨m th·∫•y nh·ªØng kh√≥a h·ªçc th√∫ v·ªã d√†nh cho b·∫°n. H√£y c√πng kh√°m ph√° nh√©:\",\n",
        "            \"üåü T·ªët qu√°! Tr·ª£ l√Ω EasyEdu ƒë√£ ph√°t hi·ªán ra c√°c kh√≥a h·ªçc tuy·ªát v·ªùi cho b·∫°n. H√£y xem ngay:\",\n",
        "            \"üéâ Nh·ªØng kh√≥a h·ªçc ph√π h·ª£p v·ªõi b·∫°n ƒë√£ ƒë∆∞·ª£c t√¨m th·∫•y! H√£y kh√°m ph√° ngay nh√©:\",\n",
        "            \"üìö Tr·ª£ l√Ω EasyEdu r·∫•t vui khi th√¥ng b√°o r·∫±ng ƒë√£ c√≥ c√°c kh√≥a h·ªçc ph√π h·ª£p v·ªõi b·∫°n. H√£y xem ngay:\",\n",
        "            \"üöÄ C√°c kh√≥a h·ªçc l√Ω t∆∞·ªüng ƒëang ch·ªù b·∫°n! H√£y c√πng kh√°m ph√° nh√©:\",\n",
        "            \"‚ú® Tr·ª£ l√Ω EasyEdu ƒë√£ t√¨m th·∫•y nh·ªØng kh√≥a h·ªçc tuy·ªát v·ªùi cho b·∫°n. H√£y xem ngay n√†o:\",\n",
        "            \"üåº Hooray! Nh·ªØng kh√≥a h·ªçc ph√π h·ª£p ƒë√£ s·∫µn s√†ng cho b·∫°n. H√£y kh√°m ph√° ngay nh√©:\",\n",
        "            \"üéà Tr·ª£ l√Ω EasyEdu r·∫•t vui khi t√¨m th·∫•y c√°c kh√≥a h·ªçc th√∫ v·ªã cho b·∫°n. H√£y c√πng xem n√†o:\",\n",
        "            \"üòç Tr·ª£ l√Ω EasyEdu ƒë√£ ph√°t hi·ªán ra nh·ªØng kh√≥a h·ªçc tuy·ªát v·ªùi d√†nh cho b·∫°n. H√£y kh√°m ph√° ngay nh√©:\",\n",
        "            \"üíñ Nh·ªØng kh√≥a h·ªçc ph√π h·ª£p v·ªõi b·∫°n ƒë√£ s·∫µn s√†ng! H√£y c√πng kh√°m ph√° v·ªõi tr·ª£ l√Ω EasyEdu nh√©:\"\n",
        "        ]\n",
        "        random_header = random.choice(header_responses)\n",
        "        random_footer = random.choice(footer_responses)\n",
        "        course_list = []\n",
        "        count=0\n",
        "        for index, course_info in matching_courses.iterrows():\n",
        "            course_name = course_info['name']\n",
        "            course_link = 'https://prse-fe.vercel.app/course-detail/' + str(course_info['id'])\n",
        "            course_price = course_info['original_price']\n",
        "            course_price= format_price(course_price)\n",
        "\n",
        "# ¬∑ Gi√° :\n",
        "# ¬∑ Link :\n",
        "\n",
        "            course_list.append(f\"**{count + 1}. {course_name}**:\"+\"\\n \"+ f\"¬∑ **Gi√° :** {course_price} VND\"+\"\\n\"+ f\"¬∑ **Link :** {course_link}\")\n",
        "            count+=1\n",
        "        all_courses = f\"{random_header}\\n\" + \"\\n\".join(course_list) + f\"\\n{random_footer}\"\n",
        "        return jsonify({'error_message': {},\n",
        "                'code': 1,\n",
        "                'data':{\n",
        "                  'message': all_courses\n",
        "                }}), 200\n",
        "\n",
        "    else:\n",
        "        no_responses = [\n",
        "            \"üåº √îi kh√¥ng! Tr·ª£ l√Ω EasyEdu kh√¥ng t√¨m th·∫•y kh√≥a h·ªçc n√†o ph√π h·ª£p v·ªõi b·∫°n. H√£y th·ª≠ l·∫°i l·∫ßn n·ªØa nh√©, c√≥ th·ªÉ ƒëi·ªÅu g√¨ ƒë√≥ tuy·ªát v·ªùi s·∫Ω xu·∫•t hi·ªán!\",\n",
        "            \"ü§ó Tr·ª£ l√Ω EasyEdu r·∫•t ti·∫øc, nh∆∞ng hi·ªán t·∫°i ch∆∞a t√¨m th·∫•y kh√≥a h·ªçc n√†o h·ª£p v·ªõi b·∫°n. ƒê·ª´ng ng·∫°i th·ª≠ l·∫°i sau m·ªôt ch√∫t nh√©!\",\n",
        "            \"üíî R·∫•t ti·∫øc, nh∆∞ng c√≥ v·∫ª nh∆∞ tr·ª£ l√Ω EasyEdu ch∆∞a t√¨m th·∫•y kh√≥a h·ªçc n√†o ph√π h·ª£p. H√£y th·ª≠ l·∫°i l·∫ßn n·ªØa, c√≥ th·ªÉ b·∫°n s·∫Ω t√¨m th·∫•y ƒëi·ªÅu m√¨nh th√≠ch!\",\n",
        "            \"üåü D√π tr·ª£ l√Ω EasyEdu ch∆∞a t√¨m th·∫•y kh√≥a h·ªçc n√†o ph√π h·ª£p, h√£y th·ª≠ l·∫°i nh√©! C√≥ th·ªÉ ƒëi·ªÅu b·∫•t ng·ªù ƒëang ch·ªù b·∫°n ·ªü l·∫ßn sau!\",\n",
        "            \"‚ú® Tr·ª£ l√Ω EasyEdu kh√¥ng t√¨m th·∫•y kh√≥a h·ªçc n√†o ph√π h·ª£p v·ªõi b·∫°n. Nh∆∞ng ƒë·ª´ng n·∫£n l√≤ng, h√£y th·ª≠ l·∫°i l·∫ßn n·ªØa nh√©!\",\n",
        "            \"üåà Tr·ª£ l√Ω EasyEdu mu·ªën gi√∫p b·∫°n, nh∆∞ng hi·ªán t·∫°i ch∆∞a c√≥ kh√≥a h·ªçc n√†o ph√π h·ª£p. H√£y quay l·∫°i v√† th·ª≠ l·∫°i m·ªôt l·∫ßn n·ªØa nh√©!\",\n",
        "            \"ü§î C√≥ v·∫ª nh∆∞ tr·ª£ l√Ω EasyEdu ch∆∞a t√¨m th·∫•y kh√≥a h·ªçc n√†o h·ª£p v·ªõi b·∫°n. H√£y th·ª≠ l·∫°i sau m·ªôt ch√∫t, bi·∫øt ƒë√¢u s·∫Ω c√≥ ƒëi·ªÅu th√∫ v·ªã!\",\n",
        "            \"üåª ƒê·ª´ng bu·ªìn nh√©! Hi·ªán t·∫°i tr·ª£ l√Ω EasyEdu ch∆∞a t√¨m th·∫•y kh√≥a h·ªçc n√†o ph√π h·ª£p, nh∆∞ng h√£y th·ª≠ l·∫°i l·∫ßn n·ªØa ƒë·ªÉ t√¨m ki·∫øm th√™m l·ª±a ch·ªçn!\",\n",
        "            \"üíñ R·∫•t ti·∫øc v√¨ tr·ª£ l√Ω EasyEdu ch∆∞a t√¨m th·∫•y kh√≥a h·ªçc n√†o cho b·∫°n. H√£y gh√© thƒÉm th∆∞·ªùng xuy√™n v√† th·ª≠ l·∫°i l·∫ßn n·ªØa nh√©!\",\n",
        "            \"üêæ M·∫∑c d√π tr·ª£ l√Ω EasyEdu ch∆∞a t√¨m th·∫•y kh√≥a h·ªçc n√†o ph√π h·ª£p, nh∆∞ng h√£y ki√™n nh·∫´n v√† th·ª≠ l·∫°i sau. Tr·ª£ l√Ω EasyEdu lu√¥n ·ªü ƒë√¢y ƒë·ªÉ h·ªó tr·ª£ b·∫°n!\"\n",
        "        ]\n",
        "        all_courses = random.choice(no_responses)\n",
        "        return jsonify({'error_message': {},\n",
        "                'code': 1,\n",
        "                'data':{\n",
        "                  'message': all_courses\n",
        "                }}), 200\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Load the data when the application starts\n",
        "    load_data()\n",
        "    print(\"API_site:\",public_url)\n",
        "    app.run(debug=False, host='0.0.0.0', port=5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d97cee155f9547c9",
      "metadata": {
        "id": "d97cee155f9547c9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "initial_id",
        "outputId": "d710e529-1daa-4c1f-f969-3e512c2b8207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    id  average_rating                                        description  \\\n",
            "0    1             5.0                              Ki·∫øn th·ª©c nh·∫≠p m√¥n IT   \n",
            "1    2             4.0                              Ki·∫øn th·ª©c nh·∫≠p m√¥n IT   \n",
            "2    2             4.0                              Ki·∫øn th·ª©c nh·∫≠p m√¥n IT   \n",
            "3    3             3.8                  Kh√≥a h·ªçc marketing online v√† SEO.   \n",
            "4    4             4.7                               L·∫≠p tr√¨nh C++ c∆° b·∫£n   \n",
            "5    5             4.9                          HTML CSS t·ª´ Zero ƒë·∫øn Hero   \n",
            "6    6             3.5             Kh√≥a h·ªçc n·∫•u ƒÉn cho ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu.   \n",
            "7    7             4.2                         Responsive V·ªõi Grid System   \n",
            "8    8             3.9           Kh√≥a h·ªçc v·ªÅ ph√¢n t√≠ch d·ªØ li·ªáu v·ªõi Excel.   \n",
            "9    9             4.4                        L·∫≠p Tr√¨nh JavaScript C∆° B·∫£n   \n",
            "10  10             4.1            Kh√≥a h·ªçc v·ªÅ l·∫≠p tr√¨nh ·ª©ng d·ª•ng di ƒë·ªông.   \n",
            "11  11             4.6                      L·∫≠p Tr√¨nh JavaScript N√¢ng Cao   \n",
            "12  12             4.3                L√†m vi·ªác v·ªõi Terminal & Ubuntu Kh√≥a   \n",
            "13  13             4.8                  X√¢y D·ª±ng Website v·ªõi ReactJS Kh√≥a   \n",
            "14  14             4.5        Kh√≥a h·ªçc mi·ªÖn ph√≠ v·ªÅ ƒë·ªì h·ªça 3D v·ªõi Blender.   \n",
            "15  15             4.1            Kh√≥a h·ªçc mi·ªÖn ph√≠ v·ªÅ k·ªπ nƒÉng giao ti·∫øp.   \n",
            "16  16             4.0  Kh√≥a h·ªçc mi·ªÖn ph√≠ v·ªÅ ph√¢n t√≠ch d·ªØ li·ªáu v·ªõi Pyt...   \n",
            "17  17             4.4         Kh√≥a h·ªçc mi·ªÖn ph√≠ v·ªÅ vi·∫øt blog th√†nh c√¥ng.   \n",
            "18  18             4.2            Kh√≥a h·ªçc mi·ªÖn ph√≠ v·ªÅ t√¢m l√Ω h·ªçc c∆° b·∫£n.   \n",
            "19  19             4.9  Kh√≥a h·ªçc mi·ªÖn ph√≠ v·ªÅ l√†m video v·ªõi Adobe Premi...   \n",
            "20  20             4.7          Kh√≥a h·ªçc mi·ªÖn ph√≠ v·ªÅ ph√°t tri·ªÉn b·∫£n th√¢n.   \n",
            "\n",
            "    original_price  course_id  sub_category_id  \n",
            "0           100000        1.0              2.0  \n",
            "1           120000        2.0              2.0  \n",
            "2           120000        2.0              1.0  \n",
            "3           120000        3.0              2.0  \n",
            "4                0        4.0              2.0  \n",
            "5                0        5.0              2.0  \n",
            "6               50        6.0              2.0  \n",
            "7                0        7.0              2.0  \n",
            "8               75        8.0              2.0  \n",
            "9                0        9.0              2.0  \n",
            "10             150        NaN              NaN  \n",
            "11               0        NaN              NaN  \n",
            "12               0        NaN              NaN  \n",
            "13               0        NaN              NaN  \n",
            "14               0        NaN              NaN  \n",
            "15               0        NaN              NaN  \n",
            "16               0        NaN              NaN  \n",
            "17               0        NaN              NaN  \n",
            "18               0        NaN              NaN  \n",
            "19               0        NaN              NaN  \n",
            "20          100000        NaN              NaN  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from underthesea import word_tokenize\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def preprocess_vietnamese(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def get_effective_price(row, courses_discount):\n",
        "    \"\"\"\n",
        "    Get the effective price considering discounts\n",
        "    \"\"\"\n",
        "    course_id = row['id']\n",
        "    original_price = row['original_price']\n",
        "\n",
        "    # Check if course has an active discount\n",
        "    discount_info = courses_discount[\n",
        "        (courses_discount['course_id'] == course_id) &\n",
        "        (courses_discount['is_active'] == True)\n",
        "    ]\n",
        "\n",
        "    if not discount_info.empty:\n",
        "        return discount_info.iloc[0]['discount_price']\n",
        "    return original_price\n",
        "\n",
        "def calculate_combined_similarity(courses, courses_discount, text_weight=0.6, rating_weight=0.2, price_weight=0.2, category_weight=0.9):\n",
        "    \"\"\"\n",
        "    Calculate similarity between courses based on description, rating, and effective price\n",
        "\n",
        "    Parameters:\n",
        "    - courses: DataFrame containing 'id', 'description', 'average_rating', and 'original_price'\n",
        "    - courses_discount: DataFrame containing discount information\n",
        "    - text_weight: weight for description similarity (default: 0.6)\n",
        "    - rating_weight: weight for rating similarity (default: 0.2)\n",
        "    - price_weight: weight for price similarity (default: 0.2)\n",
        "\n",
        "    Returns:\n",
        "    - combined similarity matrix\n",
        "    \"\"\"\n",
        "    # Preprocess description\n",
        "    courses['description'] = courses['description'].apply(preprocess_vietnamese)\n",
        "\n",
        "    # Calculate text similarity\n",
        "    tfidf = TfidfVectorizer(\n",
        "        max_features=3522,\n",
        "        ngram_range=(1, 2)\n",
        "    )\n",
        "    text_vectors = tfidf.fit_transform(courses['description'])\n",
        "    text_similarity = cosine_similarity(text_vectors)\n",
        "\n",
        "    # Calculate effective prices considering discounts\n",
        "    courses['effective_price'] = courses.apply(\n",
        "        lambda row: get_effective_price(row, courses_discount),\n",
        "        axis=1\n",
        "    )\n",
        "    # Normalize numerical features\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Handle rating similarity\n",
        "    ratings = courses[['average_rating']].values\n",
        "    normalized_ratings = scaler.fit_transform(ratings)\n",
        "    rating_similarity = 1 - np.abs(normalized_ratings - normalized_ratings.T)\n",
        "\n",
        "    # Handle price similarity using effective prices\n",
        "    prices = courses[['effective_price']].values\n",
        "    normalized_prices = scaler.fit_transform(prices)\n",
        "    price_similarity = 1 - np.abs(normalized_prices - normalized_prices.T)\n",
        "\n",
        "    # Handle category similarity: 1 if categories match, 0 if they don't\n",
        "    categories = courses[['sub_category_id']].values\n",
        "    category_similarity = np.array([[1 if cat1 == cat2 else 0 for cat2 in categories] for cat1 in categories])\n",
        "\n",
        "    # Combine all similarities with their respective weights\n",
        "    combined_similarity = (\n",
        "        text_weight * text_similarity +\n",
        "        rating_weight * rating_similarity +\n",
        "        price_weight * price_similarity +\n",
        "        category_weight * category_similarity\n",
        "    )\n",
        "\n",
        "    return combined_similarity\n",
        "\n",
        "# Load data\n",
        "courses = pd.read_csv('drive/MyDrive/Data_PRSE/course.csv')\n",
        "courses_discount = pd.read_csv('drive/MyDrive/Data_PRSE/course_discount.csv')\n",
        "courses_category = pd.read_csv('drive/MyDrive/Data_PRSE/course_category.csv')\n",
        "# Select required columns\n",
        "courses = courses[['id', 'average_rating', 'description', 'original_price']]\n",
        "courses = courses.merge(courses_category[['course_id', 'sub_category_id']], left_on='id', right_on='course_id', how='left')\n",
        "print(courses)\n",
        "# Calculate similarity matrix with effective prices\n",
        "similarity_matrix = calculate_combined_similarity(\n",
        "    courses,\n",
        "    courses_discount,\n",
        "    text_weight=0.75,\n",
        "    rating_weight=0.5,\n",
        "    price_weight=0.4\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}